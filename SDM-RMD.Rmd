---
title: "SDM II Project"
output: pdf_document
date: "2023-04-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

#Loading Necessary Libraries

library(xts)
library(readr)
library(viridis)
library(lubridate)

library(ggplot2)
library(plotly)
library(viridis)
library(reshape2)
library(zoo)
library(Quandl)
library(dygraphs)
library(shiny)
library(tidyverse)
library(forecast)
```

## Change In Volatility Over Time

| Financial risk has many faces, and we measure it in many ways, but for now, let's agree that it is a measure of the possible loss on an investment. In financial markets, where we measure prices frequently, volatility (which is analogous to *standard deviation*) is an obvious choice to measure risk. But in real markets, volatility changes with the market itself.
| 
| 

![Returns Of Four Different Assets](T-Bonds.png)

| 

The image presented depicts the returns of four distinct assets, each displaying a pattern of alternating periods of high and low volatilities. Notably, the greatest volatility occurred near the conclusion of 2008, which coincided with the most severe stage of the recent financial crisis.

```{r}

# Testing Quandl API

data <- Quandl("FED/SVENY")

```

## Daily Estimates in 2022

```{r}

head(data, 10)
```

## Exploratory Data Analysis

```{r}

na_vals <- colSums(is.na(data))

plot(na_vals, main = "NA Values Trend")

```

### Plotting 30 Year Rates To Check NAN Values

```{r}


ggplot(data, aes(x = Date, y = SVENY30)) +
  geom_line() +
  xlab("Date") +
  ylab("Rate") +
  ggtitle("30 Year Rate")
  




```

```{r}


# SVENY01

ggplot(data, aes(x = Date, y = SVENY01)) +
  geom_line() +
  xlab("Date") +
  ylab("Rate") +
  ggtitle("1 Year Rate")

```

| 
  Since the missing values were randomly distributed and accounted for a small proportion of the dataset, we decided to drop them from our analysis instead of using any imputation method, which could introduce bias. The line plot was used to confirm that there were no missing data points in between the available data.

## Dropping NA Values

```{r}

# Droopping NA Values

data <- na.omit(data)

```

```{r}



```

```{r}


plot(colSums(is.na(data)), main = "NA Values")
```

## Converting Data To Time Series

```{r}

data_xts <- as.xts(x = data[, -1], order.by = as.Date(data$Date))

#data_xts <- subset(data_xts, select = -SVENY30_missing)

```

```{r}


# Create a daily sequence of dates
daily_dates <- seq(from = start(data_xts), to = end(data_xts), by = "day")

# Merge the daily sequence of dates with the original xts object
data_xts_all_dates <- merge(data_xts, xts(order.by = daily_dates))

# Fill missing values with last observation carried forward
data_xts_all_dates_filled <- na.locf(data_xts_all_dates)


```

```{r}

```

```{r}

plot(data_xts, main = "US Zero Coupon Yields")
```

### Rates - BEFORE COVID

```{r}

# Filter

# Filter data for the year 2019
data_xts_2019 <- data_xts[format(index(data_xts), "%Y") == "2019"]
```

```{r}


```

```{r}

# Before COVID, 2019

dygraph(data_xts_2019, 
        main = "All Zero Coupon Yields (1-30) 2019 | Before COVID", 
        ylab = "Value") %>%
  dyAxis('x', axisLabelFontSize = 12) %>%
  dyRangeSelector() %>%
  dyLegend(show = "follow") 

```

### Rates - AFTER COVID

```{r}

# Filter data for the year 2022
data_xts_after <- data_xts[format(index(data_xts), "%Y") >= "2022"]
```

```{r}


dygraph(data_xts_after, 
        main = "All Zero Coupon Yields (1-30) | After COVID", 
        ylab = "Value") %>%
  dyAxis('x', axisLabelFontSize = 12) %>%
  dyRangeSelector() %>%
  dyLegend(show = "follow") 
```

## ETS EDA\

```{r}


# Filter data for the year 2022
data_xts_2018af <- data_xts[format(index(data_xts), "%Y") >= "2018"]
```

```{r}

is.Date(index(data_xts))

head(data)

```

### Decompose SVENY01 Data

```{r}

data
```

# Imputing Values

```{r}

data_treated <- na.locf(data, na.rm = FALSE)

data_treated

```

```{r}

colSums(is.na(data_treated))

head(data_treated)

```

```{r}


data <- na.omit(data)

nrow(data)

```

```{r}

data_2022 <- data %>% 
  filter(Date > '2022-01-01' & Date < '2022-12-31')



```

```{r}

data_2022
```

```{r}
dfasxts <- as.xts(x = data[, -1], order.by = data$Date)
data_2022 <- as.xts(x = data_2022[, -1], order.by = data_2022$Date)
dygraph(data_2022, main = "All Zero Coupon Yields (All Time Horizons) 2022", 
        ylab = "Value") %>%
            dyAxis('x', axisLabelFontSize = 12) %>%
            dyRangeSelector()
```

## Testing Shiny Board

```{r}

data$Date <- as.Date(data$Date)
data$year <- format(as.Date(data$Date, format="%m/%d/%Y"),"%Y")
data <- select(data, -Date)
data <- na.omit(data)

data <- data %>%
  group_by(year) %>%
  summarise_all(mean)

```

```{r}


```

```{r}

SVENY01 <- data$SVENY01
SVENY10 <- data$SVENY10
SVENY30 <- data$SVENY30


plot_data <- data.frame(data, SVENY01, SVENY10, SVENY30)
plot_data <- select(plot_data, year, SVENY01, SVENY10, SVENY30)





```

```{r}
fig <- plot_ly(plot_data, x = plot_data$year, y = ~SVENY01, name = 'SVENY01', 
               type = 'scatter', mode = 'lines + markers') 
fig <- fig %>% add_trace(y = ~SVENY10, name = 'SVENY10', mode = 'lines + markers') 
fig <- fig %>% add_trace(y = ~SVENY30, name = 'SVENY30', mode = 'lines + markers')
fig <- fig %>% layout(title = "",
         xaxis = list(title = "year"),
         yaxis = list (title = ""))


fig
```

```{r}

row.names(data) <- data$year

data <- select(data, -year)

data_mat <- data.matrix(data)

```

```{r}

data_mat

```

```{r}


heatmap(data_mat, Rowv = NA, Colv = NA, scale = "column", margins = c(4,1))
```
